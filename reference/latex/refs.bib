@InProceedings{C2,
  author = 	 "Jones, C.D. and Smith, A.B. and Roberts, E.F.",
  title =        "Article Title",
  booktitle =        "Proceedings Title",
  organization = "IEEE",
  year = 	 "2003",
  volume = 	 "II",
  pages = 	 "803-806"
}
@misc{byu-locating-bacterial-flagellar-motors-2025,
    author = {Andrew Darley and Braxton Owens and Bryan Morse and Eben Lonsdale and Gus Hart and Jackson Pond and Joshua Blaser and Matias Gomez Paz and Matthew Ward and Rachel Webb and Andrew Crowther and Nathan Smith and Grant J. Jensen and TJ Hart and Maggie Demkin and Walter Reade and Elizabeth Park},
    title = {BYU - Locating Bacterial Flagellar Motors 2025},
    year = {2025},
    howpublished = {\url{https://kaggle.com/competitions/byu-locating-bacterial-flagellar-motors-2025}},
    note = {Kaggle}
}
@article{beckSnapshotsNuclearPore2007,
  title = {Snapshots of Nuclear Pore Complexes in Action Captured by Cryo-Electron Tomography},
  author = {Beck, Martin and Luci{\'c}, Vladan and F{\"o}rster, Friedrich and Baumeister, Wolfgang and Medalia, Ohad},
  year = {2007},
  month = oct,
  journal = {Nature},
  volume = {449},
  number = {7162},
  pages = {611--615},
  issn = {1476-4687},
  doi = {10.1038/nature06170},
  abstract = {Nuclear pore complexes reside in the nuclear envelope of eukaryotic cells and mediate the nucleocytoplasmic exchange of macromolecules. Traffic is regulated by mobile transport receptors that target their cargo to the central translocation channel, where phenylalanine-glycine-rich repeats serve as binding sites. The structural analysis of the nuclear pore is a formidable challenge given its size, its location in a membranous environment and its dynamic nature. Here we have used cryo-electron tomography to study the structure of nuclear pore complexes in their functional environment, that is, in intact nuclei of Dictyostelium discoideum. A new image-processing strategy compensating for deviations of the asymmetric units (protomers) from a perfect eight-fold symmetry enabled us to refine the structure and to identify new features. Furthermore, the superposition of a large number of tomograms taken in the presence of cargo, which was rendered visible by gold nanoparticles, has yielded a map outlining the trajectories of import cargo. Finally, we have performed single-molecule Monte Carlo simulations of nuclear import to interpret the experimentally observed cargo distribution in the light of existing models for nuclear import.},
  langid = {english},
  pmid = {17851530},
  keywords = {Active Transport Cell Nucleus,Animals,Cryoelectron Microscopy,Dictyostelium,Models Molecular,Molecular Conformation,Nuclear Pore,Tomography}
}
@article{kojimaBacterialFlagellarMotor2004,
  title = {The Bacterial Flagellar Motor: Structure and Function of a Complex Molecular Machine},
  shorttitle = {The Bacterial Flagellar Motor},
  author = {Kojima, Seiji and Blair, David F.},
  year = {2004},
  journal = {International Review of Cytology},
  volume = {233},
  pages = {93--134},
  issn = {0074-7696},
  doi = {10.1016/S0074-7696(04)33003-2},
  abstract = {The bacterial flagellar motor harnesses ion flow to drive rotary motion, at speeds reaching 100000 rpm and with apparently tight coupling. The functional properties of the motor are quite well understood, but its molecular mechanism remains unknown. Studies of motor physiology, together with mutational and biochemical studies of the components, place significant constraints on the mechanism. Rotation is probably driven by conformational changes in membrane-protein complexes that form the stator. These conformational changes occur as protons move on and off a critical aspartate residue in the stator protein MotB, and the resulting forces are applied to the rotor protein FliG. The bacterial flagellum is a complex structure built from about two dozen proteins. Its construction requires an apparatus at the base that exports many flagellar components to their sites of installation by way of an axial channel through the structure. The sequence of events in assembly is understood in general terms, but not yet at the molecular level. A fuller understanding of motor rotation and flagellar assembly will require more data on the structures and organization of the constituent proteins.},
  langid = {english},
  pmid = {15037363},
  keywords = {Bacteria,Bacterial Proteins,Cell Movement,Flagella,Molecular Conformation,Molecular Motor Proteins,Rotation}
}
@article{lucicCryoelectronTomographyChallenge2013,
  title = {Cryo-Electron Tomography: The Challenge of Doing Structural Biology in Situ},
  shorttitle = {Cryo-Electron Tomography},
  author = {Lu{\v c}i{\v c}, Vladan and Rigort, Alexander and Baumeister, Wolfgang},
  year = {2013},
  month = aug,
  journal = {The Journal of Cell Biology},
  volume = {202},
  number = {3},
  pages = {407--419},
  issn = {1540-8140},
  doi = {10.1083/jcb.201304193},
  abstract = {Electron microscopy played a key role in establishing cell biology as a discipline, by producing fundamental insights into cellular organization and ultrastructure. Many seminal discoveries were made possible by the development of new sample preparation methods and imaging modalities. Recent technical advances include sample vitrification that faithfully preserves molecular structures, three-dimensional imaging by electron tomography, and improved image-processing methods. These new techniques have enabled the extraction of high fidelity structural information and are beginning to reveal the macromolecular organization of unperturbed cellular environments.},
  langid = {english},
  pmcid = {PMC3734081},
  pmid = {23918936},
  keywords = {Cryoelectron Microscopy,DNA,Imaging Three-Dimensional,Nucleic Acid Conformation,Protein Conformation,Proteins,Tomography},
  file = {F:\Zotero File\storage\MLE6TRDK\Lučič 等 - 2013 - Cryo-electron tomography the challenge of doing structural biology in situ.pdf}
}

@book{jeonInternationalReviewCytology2004,
  title = {International {{Review}} of {{Cytology}}: {{A Survey}} of {{Cell Biology}}},
  shorttitle = {International {{Review}} of {{Cytology}}},
  author = {Jeon, Kwang W.},
  year = {2004},
  month = jun,
  publisher = {Elsevier},
  abstract = {The acclaimed International Review of Cytology series presents current advances and reviews in cell biology, both plant and animal. Articles address structure and control of gene expression, nucleocytoplasmic interactions, control of cell development and differentiation, and cell transformation and growth. Authored by some of the foremost scientists in the field, each volume provides up-to-date information and directions for future research.},
  googlebooks = {9bkQk5sanhIC},
  isbn = {978-0-08-052247-0},
  langid = {english},
  keywords = {Science / Life Sciences / Cell Biology}
}

@misc{jocherUltralyticsYolov5V702022,
  title = {Ultralytics/Yolov5: V7.0 - {{YOLOv5 SOTA Realtime Instance Segmentation}}},
  shorttitle = {Ultralytics/Yolov5},
  author = {Jocher, Glenn and Chaurasia, Ayush and Stoken, Alex and Borovec, Jirka and NanoCode012 and Kwon, Yonghye and Michael, Kalen and TaoXie and Fang, Jiacong and {imyhxy} and Lorna and Yifu), 曾逸夫(Zeng and Wong, Colin and V, Abhiram and Montes, Diego and Wang, Zhiqiang and Fati, Cristi and Nadar, Jebastin and Laughing and UnglvKitDe and Sonck, Victor and {tkianai} and {yxNONG} and Skalski, Piotr and Hogan, Adam and Nair, Dhruv and Strobel, Max and Jain, Mrinal},
  year = {2022},
  month = nov,
  doi = {10.5281/zenodo.7347926},
  urldate = {2025-04-01},
  isbn = {978-3-031-08999-2},
  langid = {english},
  keywords = {Brain tumor segmentation,BRATS,Image segmentation,Swin transformer,Swin UNETR,UNETR,Vision transformer},
  file = {F:\Zotero File\storage\XDIDAHPQ\Hatamizadeh 等 - 2022 - Swin UNETR Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images.pdf}
}
@inproceedings{cicek3DUNetLearning2016,
  title = {{{3D U-Net}}: {{Learning Dense Volumetric Segmentation}} from {{Sparse Annotation}}},
  shorttitle = {{{3D U-Net}}},
  booktitle = {Medical {{Image Computing}} and {{Computer-Assisted Intervention}} -- {{MICCAI}} 2016},
  author = {{\c C}i{\c c}ek, {\"O}zg{\"u}n and Abdulkadir, Ahmed and Lienkamp, Soeren S. and Brox, Thomas and Ronneberger, Olaf},
  editor = {Ourselin, Sebastien and Joskowicz, Leo and Sabuncu, Mert R. and Unal, Gozde and Wells, William},
  year = {2016},
  pages = {424--432},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-46723-8_49},
  abstract = {This paper introduces a network for volumetric segmentation that learns from sparsely annotated volumetric images. We outline two attractive use cases of this method: (1) In a semi-automated setup, the user annotates some slices in the volume to be segmented. The network learns from these sparse annotations and provides a dense 3D segmentation. (2) In a fully-automated setup, we assume that a representative, sparsely annotated training set exists. Trained on this data set, the network densely segments new volumetric images. The proposed network extends the previous u-net architecture from Ronneberger et al. by replacing all 2D operations with their 3D counterparts. The implementation performs on-the-fly elastic deformations for efficient data augmentation during training. It is trained end-to-end from scratch, i.e., no pre-trained network is required. We test the performance of the proposed method on a complex, highly variable 3D structure, the Xenopus kidney, and achieve good results for both use cases.},
  isbn = {978-3-319-46723-8},
  langid = {english},
  keywords = {3D,Biomedical volumetric image segmentation,Convolutional neural networks,Fully-automated,Semi-automated,Sparse annotation,Xenopus kidney},
  file = {F:\Zotero File\storage\R5MNNXBH\Çiçek 等 - 2016 - 3D U-Net Learning Dense Volumetric Segmentation from Sparse Annotation.pdf}
}

@misc{yaseenWhatYOLOv8InDepth2024,
  title = {What Is {{YOLOv8}}: {{An In-Depth Exploration}} of the {{Internal Features}} of the {{Next-Generation Object Detector}}},
  shorttitle = {What Is {{YOLOv8}}},
  author = {Yaseen, Muhammad},
  year = {2024},
  month = aug,
  number = {arXiv:2408.15857},
  eprint = {2408.15857},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.15857},
  urldate = {2025-04-01},
  abstract = {This study presents a detailed analysis of the YOLOv8 object detection model, focusing on its architecture, training techniques, and performance improvements over previous iterations like YOLOv5. Key innovations, including the CSPNet backbone for enhanced feature extraction, the FPN+PAN neck for superior multi-scale object detection, and the transition to an anchor-free approach, are thoroughly examined. The paper reviews YOLOv8's performance across benchmarks like Microsoft COCO and Roboflow 100, highlighting its high accuracy and real-time capabilities across diverse hardware platforms. Additionally, the study explores YOLOv8's developer-friendly enhancements, such as its unified Python package and CLI, which streamline model training and deployment. Overall, this research positions YOLOv8 as a state-of-the-art solution in the evolving object detection field.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {F\:\\Zotero File\\storage\\CU3HZQAP\\Yaseen - 2024 - What is YOLOv8 An In-Depth Exploration of the Internal Features of the Next-Generation Object Detec.pdf;F\:\\Zotero File\\storage\\H7IVMGKP\\2408.html}
}
@article{chenConvolutionalNeuralNetworks2017,
  title = {Convolutional {{Neural Networks}} for {{Automated Annotation}} of {{Cellular Cryo-Electron Tomograms}}},
  author = {Chen, Muyuan and Dai, Wei and Sun, Stella Y. and Jonasch, Darius and He, Cynthia Y. and Schmid, Michael F. and Chiu, Wah and Ludtke, Steven J.},
  year = {2017},
  month = oct,
  journal = {Nature methods},
  volume = {14},
  number = {10},
  pages = {983--985},
  issn = {1548-7091},
  doi = {10.1038/nmeth.4405},
  urldate = {2025-04-01},
  abstract = {Cellular Electron Cryotomography (CryoET) offers the ability to look inside cells and observe macromolecules frozen in action. A primary challenge for this technique is identifying and extracting the molecular components within the crowded cellular environment. We introduce a method using neural networks to dramatically reduce the time and human effort required for subcellular annotation and feature extraction. Subsequent subtomogram classification and averaging yields in-situ structures of molecular components of interest.},
  pmcid = {PMC5623144},
  pmid = {28846087},
  file = {F:\Zotero File\storage\9USBXTGH\Chen 等 - 2017 - Convolutional Neural Networks for Automated Annotation of Cellular Cryo-Electron Tomograms.pdf}
}
@misc{liuSwinTransformerHierarchical2021,
  title = {Swin {{Transformer}}: {{Hierarchical Vision Transformer}} Using {{Shifted Windows}}},
  shorttitle = {Swin {{Transformer}}},
  author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  year = {2021},
  month = aug,
  number = {arXiv:2103.14030},
  eprint = {2103.14030},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2103.14030},
  urldate = {2025-04-01},
  abstract = {This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with {\textbackslash}textbf\{S\}hifted {\textbackslash}textbf\{win\}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at{\textasciitilde}{\textbackslash}url\{https://github.com/microsoft/Swin-Transformer\}.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {F\:\\Zotero File\\storage\\9NMAHPZU\\Liu 等 - 2021 - Swin Transformer Hierarchical Vision Transformer using Shifted Windows.pdf;F\:\\Zotero File\\storage\\29YTUNQS\\2103.html}
}
@misc{wangCSPNetNewBackbone2019,
  title = {{{CSPNet}}: {{A New Backbone}} That Can {{Enhance Learning Capability}} of {{CNN}}},
  shorttitle = {{{CSPNet}}},
  author = {Wang, Chien-Yao and Liao, Hong-Yuan Mark and Yeh, I.-Hau and Wu, Yueh-Hua and Chen, Ping-Yang and Hsieh, Jun-Wei},
  year = {2019},
  month = nov,
  number = {arXiv:1911.11929},
  eprint = {1911.11929},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1911.11929},
  urldate = {2025-04-02},
  abstract = {Neural networks have enabled state-of-the-art approaches to achieve incredible results on computer vision tasks such as object detection. However, such success greatly relies on costly computation resources, which hinders people with cheap devices from appreciating the advanced technology. In this paper, we propose Cross Stage Partial Network (CSPNet) to mitigate the problem that previous works require heavy inference computations from the network architecture perspective. We attribute the problem to the duplicate gradient information within network optimization. The proposed networks respect the variability of the gradients by integrating feature maps from the beginning and the end of a network stage, which, in our experiments, reduces computations by 20\% with equivalent or even superior accuracy on the ImageNet dataset, and significantly outperforms state-of-the-art approaches in terms of AP50 on the MS COCO object detection dataset. The CSPNet is easy to implement and general enough to cope with architectures based on ResNet, ResNeXt, and DenseNet. Source code is at https://github.com/WongKinYiu/CrossStagePartialNetworks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {F\:\\Zotero File\\storage\\6TCUBQ6T\\Wang 等 - 2019 - CSPNet A New Backbone that can Enhance Learning Capability of CNN.pdf;F\:\\Zotero File\\storage\\E55SFQX5\\1911.html}
}
@article{bochkovskiyYOLOv4OptimalSpeed2020a,
  title = {{{YOLOv4}}: {{Optimal Speed}} and {{Accuracy}} of {{Object Detection}}},
  shorttitle = {{{YOLOv4}}},
  author = {Bochkovskiy, Alexey and Wang, Chien-Yao and Liao, H.},
  year = {2020},
  month = apr,
  journal = {ArXiv},
  urldate = {2025-04-02},
  abstract = {There are a huge number of features which are said to improve Convolutional Neural Network (CNN) accuracy. Practical testing of combinations of such features on large datasets, and theoretical justification of the result, is required. Some features operate on certain models exclusively and for certain problems exclusively, or only for small-scale datasets; while some features, such as batch-normalization and residual-connections, are applicable to the majority of models, tasks, and datasets. We assume that such universal features include Weighted-Residual-Connections (WRC), Cross-Stage-Partial-connections (CSP), Cross mini-Batch Normalization (CmBN), Self-adversarial-training (SAT) and Mish-activation. We use new features: WRC, CSP, CmBN, SAT, Mish activation, Mosaic data augmentation, CmBN, DropBlock regularization, and CIoU loss, and combine some of them to achieve state-of-the-art results: 43.5\% AP (65.7\% AP50) for the MS COCO dataset at a realtime speed of {\textasciitilde}65 FPS on Tesla V100. Source code is at this https URL},
  file = {F:\Zotero File\storage\5LGWGZ29\Bochkovskiy 等 - 2020 - YOLOv4 Optimal Speed and Accuracy of Object Detection.pdf}
}
@article{Lin2016FeaturePN,
  title={Feature Pyramid Networks for Object Detection},
  author={Tsung-Yi Lin and Piotr Doll{\'a}r and Ross B. Girshick and Kaiming He and Bharath Hariharan and Serge J. Belongie},
  journal={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2016},
  pages={936-944},
  url={https://api.semanticscholar.org/CorpusID:10716717}
}
@article{Liu2018PathAN,
  title={Path Aggregation Network for Instance Segmentation},
  author={Shu Liu and Lu Qi and Haifang Qin and Jianping Shi and Jiaya Jia},
  journal={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2018},
  pages={8759-8768},
  url={https://api.semanticscholar.org/CorpusID:3698141}
}
@article{Rezatofighi2019GeneralizedIO,
  title={Generalized Intersection Over Union: A Metric and a Loss for Bounding Box Regression},
  author={Seyed Hamid Rezatofighi and Nathan Tsoi and JunYoung Gwak and Amir Sadeghian and Ian D. Reid and Silvio Savarese},
  journal={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2019},
  pages={658-666},
  url={https://api.semanticscholar.org/CorpusID:67855581}
}

@article{Hatamizadeh2022SwinUS,
  title={Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images},
  author={Ali Hatamizadeh and V. Nath and Yucheng Tang and Dong Yang and Holger R. Roth and Daguang Xu},
  journal={ArXiv},
  year={2022},
  volume={abs/2201.01266},
  url={https://api.semanticscholar.org/CorpusID:245668780}
}
@inproceedings{Vaswani2017AttentionIA,
  title={Attention is All you Need},
  author={Ashish Vaswani and Noam M. Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  booktitle={Neural Information Processing Systems},
  year={2017},
  url={https://api.semanticscholar.org/CorpusID:13756489}
}
@article{Ulyanov2016InstanceNT,
  title={Instance Normalization: The Missing Ingredient for Fast Stylization},
  author={Dmitry Ulyanov and Andrea Vedaldi and Victor S. Lempitsky},
  journal={ArXiv},
  year={2016},
  volume={abs/1607.08022},
  url={https://api.semanticscholar.org/CorpusID:16516553}
}